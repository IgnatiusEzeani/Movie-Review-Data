{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 18 - NLP Tasks and Methods.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOB1hB/E2sis1Sz+XVNmzhZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnatiusEzeani/NLP-Lecture/blob/main/Week_18_NLP_Tasks_and_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEvcqqYPD4ig"
      },
      "source": [
        "# Week 18 - Introduction to Text Classification\r\n",
        "\r\n",
        "This lab will take you through an introductory text classification task using the contents from the [AllenNLP Guide](https://guide.allennlp.org/). AllenNLP is an open source library for building deep learning models for natural language processing, developed by the Allen Institute for Artificial Intelligence.\r\n",
        "\r\n",
        "It is built on top of PyTorch and is designed to support researchers, engineers, students, etc., who wish to build high quality deep NLP models with ease. It provides high-level abstractions and APIs for common components and models in modern NLP. It also provides an extensible framework that makes it easy to run and manage NLP experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHw133awDdnO"
      },
      "source": [
        "# Uncomment below to install 'allennlp'\r\n",
        "# !pip install allennlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkUmQPLFD2ya"
      },
      "source": [
        "from typing import Dict, Iterable, List\r\n",
        "from allennlp.data import DatasetReader, Instance\r\n",
        "from allennlp.data.fields import Field, LabelField, TextField\r\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\r\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1n_HZacUXOo"
      },
      "source": [
        "## 1 What is text classification\r\n",
        "---\r\n",
        "Text classification is one of the simplest NLP tasks, where the model, given some input text, predicts a label for the text. See the figure below for an illustration.\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/Artboard.png?raw=true' />\r\n",
        "<figcaption>A basic text classification pipeline</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "\r\n",
        "There are a variety of applications of text classification, such as spam filtering, sentiment analysis, and topic detection. Some examples are shown in the table below.\r\n",
        "\r\n",
        "|Application | Description | Input | Output|\r\n",
        "| --- | --- | --- |--- |\r\n",
        "|Spam filtering |Detect and filter spam emails | Email |Spam / Not spam |\r\n",
        "|Sentiment analysis|Detect the polarity of text |Tweet, review|Positive / Negative|\r\n",
        "|Topic detection | Detect the topic of text | News article, blog post | Business / Tech / Sports|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7VC7UdxzgCp"
      },
      "source": [
        "## Defining input and output\r\n",
        "---\r\n",
        "The first step for building an NLP model is to define its input and output. In AllenNLP, each training example is represented by an Instance object. An Instance consists of one or more Fields, where each Field represents one piece of data used by your model, either as an input or an output. Fields will get converted to tensors and fed to your model. The Reading Data chapter provides more details on using Instances and Fields to represent textual data.\r\n",
        "For text classification, the input and the output are very simple. The model takes a `TextField` that represents the input text and predicts its label, which is represented by a `LabelField:`\r\n",
        "\r\n",
        "```\r\n",
        "# Input\r\n",
        "text: TextField\r\n",
        "\r\n",
        "# Output\r\n",
        "label: LabelField\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPYm6TDx0Tvy"
      },
      "source": [
        "## Reading data\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/Slice.png?raw=true' />\r\n",
        "<figcaption>Reformating text files as instances of texts and labels</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "The first step for building an NLP application is to read the dataset and represent it with some internal data structure.\r\n",
        "\r\n",
        "AllenNLP uses DatasetReaders to read the data, whose job it is to transform raw data files into Instances that match the input / output spec. Our spec for text classification is:\r\n",
        "```\r\n",
        "# Inputs\r\n",
        "text: TextField\r\n",
        "\r\n",
        "# Outputs\r\n",
        "label: LabelField\r\n",
        "```\r\n",
        "We’ll want one Field for the input and another for the output, and our model will use the inputs to predict the outputs.\r\n",
        "We assume the dataset has a simple data file format: `[text] [TAB] [label]`, for example:\r\n",
        "```\r\n",
        "I like this movie a lot! [TAB] positive\r\n",
        "This was a monstrous waste of time [TAB] negative\r\n",
        "AllenNLP is amazing [TAB] positive\r\n",
        "Why does this have to be so complicated? [TAB] negative\r\n",
        "This sentence expresses no sentiment [TAB] neutral\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp8HG9V11Otj"
      },
      "source": [
        "## Making a DatasetReader\r\n",
        "---\r\n",
        "You can implement your own DatasetReader by inheriting from the DatasetReader class. At minimum, you need to override the _read() method, which reads the input dataset and yields Instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMYwJiHUzbMZ"
      },
      "source": [
        "@DatasetReader.register('classification-tsv')\r\n",
        "class ClassificationTsvReader(DatasetReader):\r\n",
        "    def __init__(self):\r\n",
        "        self.tokenizer = SpacyTokenizer()\r\n",
        "        self.token_indexers = {'tokens': SingleIdTokenIndexer()}\r\n",
        "\r\n",
        "    def _read(self, file_path: str) -> Iterable[Instance]:\r\n",
        "        with open(file_path, 'r') as lines:\r\n",
        "            for line in lines:\r\n",
        "                text, label = line.strip().split('\\t')\r\n",
        "                text_field = TextField(self.tokenizer.tokenize(text),\r\n",
        "                                       self.token_indexers)\r\n",
        "                label_field = LabelField(label)\r\n",
        "                fields = {'text': text_field, 'label': label_field}\r\n",
        "                yield Instance(fields)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOEEuQYY2JFv"
      },
      "source": [
        "This is a minimal DatasetReader that will return a list of classification Instances when you call reader.read(file). This reader will take each line in the input file, split the text into words using a tokenizer (the SpacyTokenizer shown here relies on spaCy), and represent those words as tensors using a word id in a vocabulary we construct for you.\r\n",
        "Pay special attention to the text and label keys that are used in the fields dictionary passed to the Instance - these keys will be used as parameter names when passing tensors into your Model later.\r\n",
        "Ideally, the output label would be optional when we create the Instances, so that we can use the same code to make predictions on unlabeled data (say, in a demo), but for the rest of this chapter we’ll keep things simple and ignore that.\r\n",
        "\r\n",
        "There are lots of places where this could be made better for a more flexible and fully-featured reader; see the section on `DatasetReaders` for a deeper dive."
      ]
    }
  ]
}