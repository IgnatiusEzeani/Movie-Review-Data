{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 18 - NLP Tasks and Methods.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/3oOe59f69ZcOH2DU3zxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnatiusEzeani/NLP-Lecture/blob/main/Week_18_NLP_Tasks_and_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEvcqqYPD4ig"
      },
      "source": [
        "# Week 18 - Introduction to Text Classification\r\n",
        "\r\n",
        "This lab will take you through an introductory text classification task using the contents from the [AllenNLP Guide](https://guide.allennlp.org/). AllenNLP is an open source library for building deep learning models for natural language processing, developed by the Allen Institute for Artificial Intelligence.\r\n",
        "\r\n",
        "It is built on top of PyTorch and is designed to support researchers, engineers, students, etc., who wish to build high quality deep NLP models with ease. It provides high-level abstractions and APIs for common components and models in modern NLP. It also provides an extensible framework that makes it easy to run and manage NLP experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701DzDvbVo-_"
      },
      "source": [
        "# Section 1: Creating a classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1n_HZacUXOo"
      },
      "source": [
        "## What is text classification\r\n",
        "---\r\n",
        "Text classification is one of the simplest NLP tasks, where the model, given some input text, predicts a label for the text. See the figure below for an illustration.\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/Artboard.png?raw=true' />\r\n",
        "<figcaption>A basic text classification pipeline</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "\r\n",
        "There are a variety of applications of text classification, such as spam filtering, sentiment analysis, and topic detection. Some examples are shown in the table below.\r\n",
        "\r\n",
        "|Application | Description | Input | Output|\r\n",
        "| --- | --- | --- |--- |\r\n",
        "|Spam filtering |Detect and filter spam emails | Email |Spam / Not spam |\r\n",
        "|Sentiment analysis|Detect the polarity of text |Tweet, review|Positive / Negative|\r\n",
        "|Topic detection | Detect the topic of text | News article, blog post | Business / Tech / Sports|\r\n",
        "|Language indentification | Detect the language of text | Written text|Igbo / English / Russian|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7VC7UdxzgCp"
      },
      "source": [
        "## Defining input and output\r\n",
        "---\r\n",
        "The first step for building an NLP model is to define its input and output. In AllenNLP, each training example is represented by an Instance object. An Instance consists of one or more Fields, where each Field represents one piece of data used by your model, either as an input or an output. Fields will get converted to tensors and fed to your model. The Reading Data chapter provides more details on using Instances and Fields to represent textual data.\r\n",
        "For text classification, the input and the output are very simple. The model takes a `TextField` that represents the input text and predicts its label, which is represented by a `LabelField:`\r\n",
        "\r\n",
        "```\r\n",
        "# Input\r\n",
        "text: TextField\r\n",
        "\r\n",
        "# Output\r\n",
        "label: LabelField\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPYm6TDx0Tvy"
      },
      "source": [
        "## Reading data\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/Slice.png?raw=true' />\r\n",
        "<figcaption>Reformating text files as instances of texts and labels</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "The first step for building an NLP application is to read the dataset and represent it with some internal data structure.\r\n",
        "\r\n",
        "AllenNLP uses DatasetReaders to read the data, whose job it is to transform raw data files into Instances that match the input / output spec. Our spec for text classification is:\r\n",
        "```\r\n",
        "# Inputs\r\n",
        "text: TextField\r\n",
        "\r\n",
        "# Outputs\r\n",
        "label: LabelField\r\n",
        "```\r\n",
        "We’ll want one Field for the input and another for the output, and our model will use the inputs to predict the outputs.\r\n",
        "We assume the dataset has a simple data file format: `[text] [TAB] [label]`, for example:\r\n",
        "```\r\n",
        "I like this movie a lot! [TAB] positive\r\n",
        "This was a monstrous waste of time [TAB] negative\r\n",
        "AllenNLP is amazing [TAB] positive\r\n",
        "Why does this have to be so complicated? [TAB] negative\r\n",
        "This sentence expresses no sentiment [TAB] neutral\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp8HG9V11Otj"
      },
      "source": [
        "## Making a DatasetReader\r\n",
        "---\r\n",
        "You can implement your own DatasetReader by inheriting from the DatasetReader class. At minimum, you need to override the _read() method, which reads the input dataset and yields Instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMYwJiHUzbMZ"
      },
      "source": [
        "@DatasetReader.register('classification-tsv')\r\n",
        "class ClassificationTsvReader(DatasetReader):\r\n",
        "    def __init__(self):\r\n",
        "        self.tokenizer = SpacyTokenizer()\r\n",
        "        self.token_indexers = {'tokens': SingleIdTokenIndexer()}\r\n",
        "\r\n",
        "    def _read(self, file_path: str) -> Iterable[Instance]:\r\n",
        "        with open(file_path, 'r') as lines:\r\n",
        "            for line in lines:\r\n",
        "                text, label = line.strip().split('\\t')\r\n",
        "                text_field = TextField(self.tokenizer.tokenize(text),\r\n",
        "                                       self.token_indexers)\r\n",
        "                label_field = LabelField(label)\r\n",
        "                fields = {'text': text_field, 'label': label_field}\r\n",
        "                yield Instance(fields)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOEEuQYY2JFv"
      },
      "source": [
        "This is a minimal DatasetReader that will return a list of classification Instances when you call reader.read(file). This reader will take each line in the input file, split the text into words using a tokenizer (the SpacyTokenizer shown here relies on spaCy), and represent those words as tensors using a word id in a vocabulary we construct for you.\r\n",
        "Pay special attention to the text and label keys that are used in the fields dictionary passed to the Instance - these keys will be used as parameter names when passing tensors into your Model later.\r\n",
        "Ideally, the output label would be optional when we create the Instances, so that we can use the same code to make predictions on unlabeled data (say, in a demo), but for the rest of this chapter we’ll keep things simple and ignore that.\r\n",
        "\r\n",
        "There are lots of places where this could be made better for a more flexible and fully-featured reader; see the section on `DatasetReaders` for a deeper dive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIrmtkivwudj"
      },
      "source": [
        "## Designing your model\r\n",
        "---\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/batch_model_loss.png?raw=true' />\r\n",
        "<figcaption>The Batch-Model-Loss structure</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "The next thing we need is a Model that will take a batch of Instances, predict the outputs from the inputs, and compute a loss.\r\n",
        "Remember that our Instances have this input/output spec:\r\n",
        "```\r\n",
        "# Inputs\r\n",
        "text: TextField\r\n",
        "\r\n",
        "# Outputs\r\n",
        "label: LabelField\r\n",
        "```\r\n",
        "Also, remember that we used these names (text and label) for the fields in the DatasetReader. AllenNLP passes those fields by name to the model code, so we need to use the same names in our model.\r\n",
        "\r\n",
        "### What should our model do?\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/text_model_labels.png?raw=true' />\r\n",
        "<figcaption>Expanded Batch-Model-Loss structure</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "Conceptually, a generic model for classifying text does the following:\r\n",
        "  - Get some features corresponding to each word in your input\r\n",
        "  - Combine those word-level features into a document-level feature vector\r\n",
        "  - Classify that document-level feature vector into one of your labels.\r\n",
        "\r\n",
        "The `allennlp` library makes each of these conceptual steps into a generic abstraction that you can use in your code, so that you can have a very flexible model that can use different concrete components for each step.\r\n",
        "\r\n",
        "### Representing text with token IDs\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/tokenIDs_models_labels.png?raw=true' />\r\n",
        "<figcaption>Token IDs are often used as initial inputs</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "The first step is changing the strings in the input text into token ids. This is handled by the SingleIdTokenIndexer that we used previously, during part of our data processing pipeline that you don’t have to write code for\r\n",
        "\r\n",
        "### Embedding tokens\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/embedding_tokens_labels.png?raw=true' />\r\n",
        "<figcaption>Token IDs are often used as initial inputs</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "The first thing the `Model` does is apply an `Embedding` function that converts each token ID that we got as input into a vector. This gives us a vector for each input token, so we have a large tensor here.\r\n",
        "\r\n",
        "## Apply Seq2Vec encoder\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/apply_seq2seq_encoder.png?raw=true' />\r\n",
        "<figcaption>Token IDs are often used as initial inputs</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "Next we apply some function that takes the sequence of vectors for each input token and squashes it into a single vector. Before the days of pretrained language models like BERT, this was typically an LSTM or convolutional encoder. With BERT we might just take the embedding of the `[CLS]` token.\r\n",
        "\r\n",
        "### Computing distribution over labels\r\n",
        "\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src='https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/compute_distributions_over_labels.png?raw=true' />\r\n",
        "<figcaption>Token IDs are often used as initial inputs</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "Finally, we take that single feature vector (for each Instance in the batch), and classify it as a label, which will give us a categorical probability distribution over our label space.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLoz0S5NNBVS"
      },
      "source": [
        "\r\n",
        "## Implementing the model - the constructor\r\n",
        "---\r\n",
        "### AllenNLP Model basics\r\n",
        "<figure>\r\n",
        "<center>\r\n",
        "<img src= 'https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/img/allennlp_model_basic.png?raw=true' />\r\n",
        "<figcaption>Token IDs are often used as initial inputs</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "Now that we know what our model is going to do, we need to implement it. First, we’ll say a few words about how `Models` work in `AllenNLP`:\r\n",
        " - An AllenNLP `Model` is just a PyTorch `Module`\r\n",
        " - It implements a `forward()` method, and requires the output to be a _dictionary_.\r\n",
        " - Its output contains a loss key during training, which is used to optimize the model\r\n",
        "\r\n",
        "Our training loop takes a batch of `Instances`, passes it through `Model.forward()`, grabs the loss `key` from the resulting dictionary, and uses backprop to compute gradients and update the model’s parameters. You don’t have to implement the training loop—all this will be taken care of by AllenNLP (though you can if you want to).\r\n",
        "\r\n",
        "### Constructing the Model\r\n",
        "\r\n",
        "In the `Model` constructor, we need to instantiate all of the parameters that we will want to train. It is often better to take most of these parameters as constructor _arguments_, so that we can configure the behavior of our model without changing the model code itself, and so that we can think at a higher level about what our model is doing. The constructor, `SimpleClassifier(Model)` for our text classification model looks like this:\r\n",
        "\r\n",
        "```python\r\n",
        "@Model.register('simple_classifier')\r\n",
        "class SimpleClassifier(Model):\r\n",
        "    def __init__(self,\r\n",
        "                 vocab: Vocabulary,\r\n",
        "                 embedder: TextFieldEmbedder,\r\n",
        "                 encoder: Seq2VecEncoder):\r\n",
        "        super().__init__(vocab)\r\n",
        "        self.embedder = embedder\r\n",
        "        self.encoder = encoder\r\n",
        "        num_labels = vocab.get_vocab_size(\"labels\")\r\n",
        "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\r\n",
        "```\r\n",
        "Notice that type annotations are used a lot in AllenNLP code - this is both for code readability (it’s _way_ easier to understand what a method does if you know the types of its arguments, instead of just their names), and because we use these annotations to do some magic for you in some cases.\r\n",
        "\r\n",
        "One of those cases is constructor parameters, where we can automatically construct the embedder and encoder from a configuration file using these type annotations. See the chapter on configuration files for more information. That chapter will also tell you about the call to `@Model.register()`.\r\n",
        "\r\n",
        "The upshot is that if you’re using the allennlp train command with a configuration file (which we show how to do below), you won’t ever have to call this constructor, it all gets taken care of for you.\r\n",
        "\r\n",
        "### Passing the vocabulary\r\n",
        "`Vocabulary` manages mappings between vocabulary items (such as words and labels) and their integer IDs. In our prebuilt training loop, the vocabulary gets created by AllenNLP after reading your training data, then passed to the `Model` when it gets constructed. We’ll find all tokens and labels that you use and assign them all integer IDs in separate namespaces. The way that this happens is fully configurable.\r\n",
        "\r\n",
        "What we did in the DatasetReader will put the labels in the default “labels” namespace, and we grab the number of labels from the vocabulary on line 10.\r\n",
        "\r\n",
        "### Embedding words\r\n",
        "\r\n",
        "To get an initial word embedding, we use AllenNLP’s `TextFieldEmbedder`. This abstraction takes the tensors created by a `TextField` and embeds each one. This is our most complex abstraction, because there are a lot of ways to do this particular operation in NLP, and we want to be able to switch between these without changing our code. More details could be found from [Representing Text as Features](https://guide.allennlp.org/representing-text-as-features).\r\n",
        "\r\n",
        "All you need to know for now is that you apply this to the `text` parameter you get in `forward()`, and you get out a tensor that has a single embedding vector for each input token, with shape (`batch_size, num_tokens, embedding_dim`).\r\n",
        "\r\n",
        "### Applying a Seq2VecEncoder\r\n",
        "\r\n",
        "To squash our sequence of token vectors into a single vector, we use AllenNLP’s `Seq2VecEncoder` abstraction. As the name implies, this encapsulates an operation that takes a sequence of vectors and returns a single vector.\r\n",
        "\r\n",
        "Because all of our modules operate on batched input, this will take a tensor shaped like (`batch_size, num_tokens, embedding_dim`) and return a tensor shaped like (`batch_size, encoding_dim`).\r\n",
        "\r\n",
        "## Applying a classification layer\r\n",
        "The final parameters our `Model` needs is a classification layer, which can transform the output of our `Seq2VecEncoder` into logits, one value per possible label. These values will be converted to a probability distribution later and used for calculating the loss.\r\n",
        "\r\n",
        "We don’t need to take the `num_labels` as a constructor argument, because we’ll just use a simple linear layer, which has sizes that we can figure out inside the constructor - the `Seq2VecEncoder` knows its output dimension, and the `Vocabulary` knows how many labels there are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6NVZMtxMAIw"
      },
      "source": [
        "## Implementing the model — the forward method\r\n",
        "\r\n",
        "Next, we need to implement the forward() method of your model, which takes the input, produces the prediction, and computes the loss. Remember, our constructor and input/output spec look like:\r\n",
        "\r\n",
        "```python\r\n",
        "@Model.register('simple_classifier')\r\n",
        "class SimpleClassifier(Model):\r\n",
        "    def __init__(self,\r\n",
        "                 vocab: Vocabulary,\r\n",
        "                 embedder: TextFieldEmbedder,\r\n",
        "                 encoder: Seq2VecEncoder):\r\n",
        "        super().__init__(vocab)\r\n",
        "        self.embedder = embedder\r\n",
        "        self.encoder = encoder\r\n",
        "        num_labels = vocab.get_vocab_size(\"labels\")\r\n",
        "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\r\n",
        "```\r\n",
        "```\r\n",
        "# Inputs:\r\n",
        "text: TextField\r\n",
        "\r\n",
        "# Outputs:\r\n",
        "label: LabelField\r\n",
        "```\r\n",
        "\r\n",
        "Here we’ll show how to use these parameters inside of `Model.forward()`, which will get arguments that match our input/output spec (because that’s how we coded the `DatasetReader`).\r\n",
        "\r\n",
        "## Model.forward()\r\n",
        "In `forward`, we use the parameters that we created in our constructor to transform the inputs into outputs. After we’ve predicted the outputs, we compute some loss function based on how close we got to the true outputs, and then return that loss (along with whatever else we want) so that we can use it to train the parameters.\r\n",
        "\r\n",
        "```python\r\n",
        "class SimpleClassifier(Model):\r\n",
        "    def forward(self,\r\n",
        "                text: TextFieldTensors,\r\n",
        "                label: torch.Tensor) -> Dict[str, torch.Tensor]:\r\n",
        "        # Shape: (batch_size, num_tokens, embedding_dim)\r\n",
        "        embedded_text = self.embedder(text)\r\n",
        "        # Shape: (batch_size, num_tokens)\r\n",
        "        mask = util.get_text_field_mask(text)\r\n",
        "        # Shape: (batch_size, encoding_dim)\r\n",
        "        encoded_text = self.encoder(embedded_text, mask)\r\n",
        "        # Shape: (batch_size, num_labels)\r\n",
        "        logits = self.classifier(encoded_text)\r\n",
        "        # Shape: (batch_size, num_labels)\r\n",
        "        probs = torch.nn.functional.softmax(logits)\r\n",
        "        # Shape: (1,)\r\n",
        "        loss = torch.nn.functional.cross_entropy(logits, label)\r\n",
        "        return {'loss': loss, 'probs': probs}\r\n",
        "```\r\n",
        "\r\n",
        "### Inputs to forward()\r\n",
        "The first thing to notice is the inputs to this function. The way the AllenNLP training loop works is that we will take the field names that you used in your `DatasetReader` and give you a batch of instances with _those same field names_ in `forward`. So, because we used `text` and `label` as our field names, we need to name our arguments to `forward` the same way.\r\n",
        "\r\n",
        "Second, notice the types of these arguments. Each type of `Field` knows how to convert itself into a `torch.Tensor`, then create a batched `torch.Tenso`r from all of the Fields with the same name from a batch of `Instances`. The types you see for `text` and `label` are the tensors produced by `TextField` and `LabelField` (again, see our chapter on using TextFields for more information about TextFieldTensors). The important part to know is that our `TextFieldEmbedder`, which we created in the constructor, expects this type of object as input and will return an embedded tensor as output.\r\n",
        "\r\n",
        "### Embedding the text\r\n",
        "\r\n",
        "The first actual modeling operation that we do is embed the text, getting a vector for each input token. Notice here that we’re not specifying anything about `how` that operation is done, just that a `TextFieldEmbedder` that we got in our constructor is going to do it. This lets us be very flexible later, changing between various kinds of embedding methods or pretrained representations (including ELMo and BERT) without changing our model code. More on this later.\r\n",
        "\r\n",
        "### Applying a Seq2VecEncoder\r\n",
        "\r\n",
        "After we have embedded our text, we next have to squash the sequence of vectors (one per token) into a single vector for the whole text. We do that using the `Seq2VecEncoder` that we got as a constructor argument. In order to behave properly when we’re batching pieces of text together that could have different lengths, we need to _mask_ elements in the `embedded_text` tensor that are only there due to padding. We use a utility function to get a mask from the `TextField` output, then pass that mask into the encoder.\r\n",
        "\r\n",
        "At the end of these lines, we have a single vector for each instance in the batch.\r\n",
        "\r\n",
        "### Making predictions\r\n",
        "\r\n",
        "The last step of our model is to take the vector for each instance in the batch and predict a label for it. Our `classifier` is a `torch.nn.Linear` layer that gives a score (commonly called a `logit`) for each possible label. We normalize those scores using a `softmax` operation to get a probability distribution over labels that we can return to a consumer of this model. For computing the loss, PyTorch has a built in function that computes the cross entropy between the logits that we predict and the true label distribution, and we use that as our loss function.\r\n",
        "\r\n",
        "And that’s it! This is all you need for a simple classifier. After you’ve written a `DatasetReader` and `Model`, AllenNLP takes care of the rest: connecting your input files to the dataset reader, intelligently batching together your instances and feeding them to the model, and optimizing the model’s parameters by using backprop on the loss. We go over this part in the next chapter.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdoLQUEdZ286"
      },
      "source": [
        "# Section 2: Training and prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmzFlYcSbHl_"
      },
      "source": [
        "The last section presented an overview of the steps involved in counstructing a classification model using a PyTorch library, `allennlp` by AllenNLP. We learned how to write your own dataset reader and construct model.\r\n",
        "\r\n",
        "In this chapter, we are going to train the text classification model and make predictions for new inputs. At this point, there are two ways to proceed: \r\n",
        "\r\n",
        "1. we can write your own script to construct the dataset reader and model and run the training loop,\r\n",
        "2. or we can write a configuration file and use the `allennlp` train command. \r\n",
        "\r\n",
        "We will explore both ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab3RlTYJc2GM"
      },
      "source": [
        "## Training the model with your own script\r\n",
        "---\r\n",
        "In this section we’ll put together a simple example of reading in data, feeding it to the model, and training the model, using your own python script instead of `allennlp train`. While we recommend using `allennlp train` for most use cases, it’s easier to understand the introduction to the training loop in this section. Once you get a handle on this, switching to using our built in command should be easy, if you want to.\r\n",
        "\r\n",
        "### The Dataset\r\n",
        "Before proceeding, here are a few words about the dataset we will use throughout this chapter. The dataset is derived from the [Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/), collections of movie reviews on IMDb along with their polarity. The labels are binary (positive and negative), and our task is to predict the label from the review text.\r\n",
        "\r\n",
        "This section is going to give a series of executable examples, that you can run yourself in your browser and see what they output. They will build on each other, with code from previous examples ending up in the `Setup` block in subsequent examples.\r\n",
        "\r\n",
        "### Testing your dataset reader\r\n",
        "In the first example, we’ll simply instantiate the dataset reader, read the movie review dataset using it, and inspect the AllenNLP Instances produced by the dataset reader. Below we have code that you can run (and modify if you want)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNTZUvKkaj0B"
      },
      "source": [
        "Uncomment below to install `allennlp`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHw133awDdnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4900eae6-a76b-4a87-e366-9069874f7cba"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/bd/c75fa01e3deb9322b637fe0be45164b40d43747661aca9195b5fb334947c/allennlp-2.1.0-py3-none-any.whl (585kB)\n",
            "\u001b[K     |████████████████████████████████| 593kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/c8/b5aac643697038ef6eb8c11c73b9ee9c2dc8cb2bc95cda2d4ee656167644/boto3-1.17.17-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.8.2+cu101)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 17.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting transformers<4.4,>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 39.8MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch<1.8.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.7.1+cu101)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (53.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/fb/7ea265e28306dde068c74e6792affd4df43e51784384829c69142042ad56/botocore-1.20.17-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 39.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.9.0,>=0.8.1->allennlp) (7.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (3.7.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.17->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<4.4,>=4.1->allennlp) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.4,>=4.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.4,>=4.1->allennlp) (2.4.7)\n",
            "Building wheels for collected packages: jsonnet, overrides, sacremoses\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388784 sha256=2c43da9f72b3c651a2b80db4980f06b2fbeb70eedf85b95d1f857b49a7308a9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=72e30bf86096d30ac69768268b97fbffc6ff58b38ff71e2b18b56d070cf387b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=744b78fdd8ee20155bc8d95ad5a3762d3e8c30f1e9da25acd6dc8681bde67f14\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built jsonnet overrides sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.17 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jsonnet, jmespath, botocore, s3transfer, boto3, overrides, sentencepiece, sacremoses, tokenizers, transformers, tensorboardX, jsonpickle, allennlp\n",
            "Successfully installed allennlp-2.1.0 boto3-1.17.17 botocore-1.20.17 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 overrides-3.1.0 s3transfer-0.3.4 sacremoses-0.0.43 sentencepiece-0.1.95 tensorboardX-2.1 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9gO5ByTewoF"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkUmQPLFD2ya"
      },
      "source": [
        "from typing import Dict, Iterable, List\r\n",
        "from allennlp.data import DatasetReader, Instance\r\n",
        "from allennlp.data.fields import Field, LabelField, TextField\r\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\r\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Adepz-hW_M"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JIJjoRYiRKm"
      },
      "source": [
        "!wget https://github.com/IgnatiusEzeani/NLP-Lecture/blob/main/reformated_sample.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnNGzrwke956"
      },
      "source": [
        "### The `ClassificationTsvReader`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "A6Jf_jvyfX_B",
        "outputId": "e6b4a5bf-4bf2-4f7d-91c4-47b2e41d8a53"
      },
      "source": [
        "class ClassificationTsvReader(DatasetReader):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        tokenizer: Tokenizer = None,\r\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\r\n",
        "        max_tokens: int = None,\r\n",
        "        **kwargs\r\n",
        "    ):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.tokenizer = tokenizer or WhitespaceTokenizer()\r\n",
        "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\r\n",
        "        self.max_tokens = max_tokens\r\n",
        "\r\n",
        "    def _read(self, file_path: str) -> Iterable[Instance]:\r\n",
        "        with open(file_path, \"r\") as lines:\r\n",
        "            for line in lines:\r\n",
        "                text, sentiment = line.strip().split(\"||\")\r\n",
        "                tokens = self.tokenizer.tokenize(text)\r\n",
        "                if self.max_tokens:\r\n",
        "                    tokens = tokens[: self.max_tokens]\r\n",
        "                text_field = TextField(tokens, self.token_indexers)\r\n",
        "                label_field = LabelField(sentiment)\r\n",
        "                fields: Dict[str, Field] = {\"text\": text_field, \"label\": label_field}\r\n",
        "                yield Instance(fields)\r\n",
        "\r\n",
        "\r\n",
        "dataset_reader = ClassificationTsvReader(max_tokens=64)\r\n",
        "instances = list(dataset_reader.read('reformated_sample.tsv'))\r\n",
        "\r\n",
        "for instance in instances[:10]:\r\n",
        "    print(instance)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a78470913b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdataset_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationTsvReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reformated_sample.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/data/dataset_readers/dataset_reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0man\u001b[0m \u001b[0miterator\u001b[0m \u001b[0mof\u001b[0m \u001b[0minstances\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_worker_islice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# If not running in a subprocess, it's safe to apply the token_indexers right away.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a78470913b7e>\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"||\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmyOh9LZqRMC"
      },
      "source": [
        "def readfile(self, file_path: str):\r\n",
        "  with open(file_path, \"r\") as lines:\r\n",
        "    for line in lines:\r\n",
        "      text, sentiment = line.strip().split(\"||\")\r\n",
        "      tokens = self.tokenizer.tokenize(text)\r\n",
        "      if self.max_tokens:\r\n",
        "          tokens = tokens[: self.max_tokens]\r\n",
        "      text_field = TextField(tokens, self.token_indexers)\r\n",
        "      label_field = LabelField(sentiment)\r\n",
        "      fields: Dict[str, Field] = {\"text\": text_field, \"label\": label_field}\r\n",
        "      yield Instance(fields)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKV0BcuAfyZc"
      },
      "source": [
        "When you run the code snippet above, you should see the dumps of the first ten instances and their content, including their text and label fields. (Note that we are only showing the first 64 tokens per instance by specifying max_tokens=64).\r\n",
        "\r\n",
        "This is one way to check if your dataset reader is working as expected. We strongly recommend writing some simple tests for your data processing code, to be sure it’s actually doing what you want it to."
      ]
    }
  ]
}